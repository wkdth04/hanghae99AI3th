{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wkdth04/hanghae99AI3th/blob/%EA%B3%BC%EC%A0%9C/(%EC%B5%9C%EC%A2%85)2%EC%A3%BC%EC%B0%A8_Transformer%EC%8B%A4%EC%8A%B5_%EC%9E%A5%EC%86%8C%EC%97%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CX9-Gbyr7cN"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1X7RM2du1zcr",
        "outputId": "4b205f1e-d6c9-4a3a-cd04-abe8d4245e38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.11/dist-packages (0.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacremoses) (2024.11.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# 토큰화 및 텍스트 전처리 라이브러리로 Hugging Face Transformers라이브러리에서 사용하는 Moses Tokenizer기반 동작\n",
        "!pip install datasets sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOdhoBVA1zcu",
        "outputId": "b754f89e-ff8f-44e5-86da-32c831b83aca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizerFast\n",
        "#추가\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer,\n",
        ")\n",
        "\n",
        "\n",
        "# ds = load_dataset(\"stanfordnlp/imdb\") -> IMDB 영화 리뷰 데이터 셋을 가져옴/로드함\n",
        "train_ds = load_dataset(\"stanfordnlp/imdb\", split=\"train\")\n",
        "test_ds = load_dataset(\"stanfordnlp/imdb\", split=\"test\")\n",
        "\n",
        "#BertTokenizerFast를 사용하여 텍스트를 토큰화(BERT토크나이저)\n",
        "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')\n",
        "\n",
        "# collate_fn 수정 — 마지막 단어 예측용으로 변형\n",
        "def collate_fn(batch):\n",
        "  max_len = 400\n",
        "  texts, labels = [], []\n",
        "\n",
        "  for row in batch:\n",
        "    tokenized = tokenizer(row['text'], truncation=True, max_length=max_len, add_special_tokens=False).input_ids\n",
        "    if len(tokenized) < 2:\n",
        "      continue\n",
        "    labels.append(tokenized[-1])  # 마지막 단어 (진짜 단어)\n",
        "    texts.append(torch.LongTensor(tokenized[:-1]))  # 마지막 단어 제외하고 입력\n",
        "\n",
        "  if len(texts) == 0:\n",
        "    return torch.LongTensor([[tokenizer.pad_token_id]]), torch.LongTensor([0])\n",
        "\n",
        "  texts = pad_sequence(texts, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "  labels = torch.LongTensor(labels)\n",
        "\n",
        "  return texts, labels\n",
        "'''2차수정\n",
        "def collate_fn(batch): ### 여기가 수정부분\n",
        "  max_len = 400\n",
        "  texts, labels = [], []\n",
        "\n",
        "  for row in batch:\n",
        "    tokenized = tokenizer(row['text'], truncation=True, max_length=max_len).input_ids\n",
        "    labels.append(tokenized[-1])  # 마지막 token을 예측 대상\n",
        "    texts.append(torch.LongTensor(tokenized[:-1]))  # 마지막 token 제거한 input\n",
        "                                                    # 마지막 token 하나만 정답(label)로 사용\n",
        "\n",
        "  texts = pad_sequence(texts, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "  labels = torch.LongTensor(labels)\n",
        "\n",
        "  return texts, labels'''\n",
        "'''1차수정\n",
        "#collate_fn을 정의하여 배치별 데이터 처리\n",
        "def collate_fn(batch):\n",
        "  max_len = 400\n",
        "  texts, labels = [], []\n",
        "  for row in batch:\n",
        "    labels.append(row['label'])\n",
        "    texts.append(row['text'])\n",
        "\n",
        "  texts = torch.LongTensor(tokenizer(texts, padding=True, truncation=True, max_length=max_len).input_ids)\n",
        "  labels = torch.LongTensor(labels)\n",
        "\n",
        "  return texts, labels\n",
        "'''\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_ds, batch_size=64, shuffle=False, collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-FshZcTZBQ2"
      },
      "source": [
        "## Self-attention\n",
        "\n",
        "이번에는 self-attention을 구현해보겠습니다.\n",
        "Self-attention은 shape이 (B, S, D)인 embedding이 들어왔을 때 attention을 적용하여 새로운 representation을 만들어내는 module입니다.\n",
        "여기서 B는 batch size, S는 sequence length, D는 embedding 차원입니다.\n",
        "구현은 다음과 같습니다.\n",
        "\n",
        "\n",
        "(추가)[참고]\n",
        "TextClassifier가 사용하는 SelfAttention 클래스는\n",
        "TransformerLayer 클래스 안에서 사용되고,\n",
        "TransformerLayer는 SelfAttention에 의존합니다.\n",
        "\n",
        "아래 순서로 진행\n",
        "1.SelfAttention 클래스 정의\n",
        "2.TransformerLayer 클래스 정의\n",
        "3.TextClassifier 클래스 정의\n",
        "4.model = TextClassifier(...) 실행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBlMVMZcRAxv"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from math import sqrt\n",
        "\n",
        "# SelfAttention 클래스 정의\n",
        "#Single-head Self-Attentionx\n",
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, input_dim, d_model):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_dim = input_dim\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.wq = nn.Linear(input_dim, d_model)\n",
        "    self.wk = nn.Linear(input_dim, d_model)\n",
        "    self.wv = nn.Linear(input_dim, d_model)\n",
        "    self.dense = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "#쿼리(Q), 키(K), 값(V)를 생성하고, 스케일링 후 소프트맥스를 적용하여 가중합을 구한다\n",
        "  def forward(self, x, mask):\n",
        "    q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
        "    score = torch.matmul(q, k.transpose(-1, -2)) # (B, S, D) * (B, D, S) = (B, S, S)\n",
        "    score = score / sqrt(self.d_model) #sqrt(d_model)으로 나눠서 값 안정화\n",
        "\n",
        "    if mask is not None:\n",
        "      score = score + (mask * -1e9)\n",
        "\n",
        "    #softmax로 attention weight 생성\n",
        "    score = self.softmax(score)\n",
        "    result = torch.matmul(score, v) #weighted sum 결과\n",
        "    result = self.dense(result) #다른 차원으로 다시매핑\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S0vMp85ZRNO"
      },
      "source": [
        "대부분은 Transformer 챕터에서 배운 수식들을 그대로 구현한 것에 불과합니다.\n",
        "차이점은 `mask`의 존재여부입니다.\n",
        "이전 챕터에서 우리는 가변적인 text data들에 padding token을 붙여 하나의 matrix로 만든 방법을 배웠습니다.\n",
        "실제 attention 계산에서는 이를 무시해주기 위해 mask를 만들어 제공해주게 됩니다.\n",
        "여기서 mask의 shape은 (B, S, 1)로, 만약 `mask[i, j] = True`이면 그 변수는 padding token에 해당한다는 뜻입니다.\n",
        "이러한 값들을 무시해주는 방법은 shape이 (B, S, S)인 `score`가 있을 때(수업에서 배운 $A$와 동일) `score[i, j]`에 아주 작은 값을 더해주면 됩니다. 아주 작은 값은 예를 들어 `-1000..00 = -1e9` 같은 것이 있습니다.\n",
        "이렇게 작은 값을 더해주고 나면 softmax를 거쳤을 때 0에 가까워지기 때문에 weighted sum 과정에서 padding token에 해당하는 `v` 값들을 무시할 수 있게 됩니다.\n",
        "\n",
        "다음은 self-attention과 feed-forward layer를 구현한 모습입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZHPCn9AS5Gp"
      },
      "outputs": [],
      "source": [
        "#Transformer Layer\n",
        "#Self-Attention을 적용한 후 피드포워드 네트워크를 통과\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "  def __init__(self, input_dim, d_model, dff):\n",
        "    super().__init__()\n",
        "\n",
        "    self.sa = SelfAttention(input_dim, d_model)\n",
        "    self.ffn = nn.Sequential( #ffn -> feed-forward 네트워크\n",
        "      nn.Linear(d_model, dff),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(dff, d_model)\n",
        "    )\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    x = self.sa(x, mask)\n",
        "    x = self.ffn(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_xC9BQJaU4q"
      },
      "source": [
        "보시다시피 self-attention의 구현이 어렵지, Transformer layer 하나 구현하는 것은 수업 때 다룬 그림과 크게 구분되지 않는다는 점을 알 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3VYrqTJagS1"
      },
      "source": [
        "## Positional encoding\n",
        "\n",
        "이번에는 positional encoding을 구현합니다. Positional encoding의 식은 다음과 같습니다:\n",
        "$$\n",
        "\\begin{align*} PE_{pos, 2i} &= \\sin\\left( \\frac{pos}{10000^{2i/D}} \\right), \\\\ PE_{pos, 2i+1} &= \\cos\\left( \\frac{pos}{10000^{2i/D}} \\right).\\end{align*}\n",
        "$$\n",
        "\n",
        "이를 Numpy로 구현하여 PyTorch tensor로 변환한 모습은 다음과 같습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Uf_jMQWDUR79",
        "outputId": "b04fa916-d805-4577-cc99-b9ce11c916a9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nprint(positional_encoding(max_len, 256).shape)'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Positional Encoding 정의\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "#위치 정보를 반영하기 위해 사인 및 코사인 함수를 활용한 포지셔널 인코딩을 추가\n",
        "#위치별로 주기적 신호(사인/코사인)을 다르게 넣어줘서 모델이 위치를 감지할 수 있게 함\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, None], np.arange(d_model)[None, :], d_model)\n",
        "    #짝수인덱스 : sin(사인)\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    #홀수인덱스 : cos(코사인)\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[None, ...]\n",
        "\n",
        "    return torch.FloatTensor(pos_encoding)\n",
        "\n",
        "\n",
        "max_len = 400 # 이 값도 모델과 함께 사용\n",
        "\n",
        "'''\n",
        "print(positional_encoding(max_len, 256).shape)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5unoDcBva3eN"
      },
      "source": [
        "Positional encoding은 `angle_rads`를 구현하는 과정에서 모두 구현이 되었습니다. 여기서 `angle_rads`의 shape은 (S, D)입니다.\n",
        "우리는 일반적으로 batch로 주어지는 shape이 (B, S, D)인 tensor를 다루기 때문에 마지막에 None을 활용하여 shape을 (1, S, D)로 바꿔주게됩니다.\n",
        "\n",
        "위에서 구현한 `TransformerLayer`와 positional encoding을 모두 합친 모습은 다음과 같습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "8MaiCGh8TsDH",
        "outputId": "1de0add6-8d8c-439c-8a45-d29b411694de"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nclass TextClassifier(nn.Module):\\n  def __init__(self, vocab_size, d_model, n_layers, dff):\\n    super().__init__()\\n\\n    self.vocab_size = vocab_size\\n    self.d_model = d_model\\n    self.n_layers = n_layers\\n    self.dff = dff\\n\\n    self.embedding = nn.Embedding(vocab_size, d_model)\\n    #학습되지 않는 파라미터로 위치정보 고정\\n    self.pos_encoding = nn.parameter.Parameter(positional_encoding(max_len, d_model), requires_grad=False)\\n    self.layers = nn.ModuleList([TransformerLayer(d_model, d_model, dff) for _ in range(n_layers)])\\n    self.classification = nn.Linear(d_model, 1)\\n\\n  def forward(self, x):\\n    mask = (x == tokenizer.pad_token_id)\\n    mask = mask[:, None, :]\\n    seq_len = x.shape[1]\\n\\n    #정적인 positional encoding을 임베딩에 더해서 사용 \\n    #이 방식은 학습 없이도 순서 정보가 모델에 주입됨\\n    x = self.embedding(x)\\n    x = x * sqrt(self.d_model)\\n    x = x + self.pos_encoding[:, :seq_len]\\n\\n    for layer in self.layers:\\n      x = layer(x, mask)\\n\\n    x = x[:, 0]\\n    x = self.classification(x)\\n\\n    return x\\n\\n\\nmodel = TextClassifier(len(tokenizer), 32, 2, 32)'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Text Classifier정의\n",
        "#임베딩, 포지셔널 인코딩, Transformer 레이어, 그리고 최종분류 층을 포함한 텍스트 분류 모델을 정의\n",
        "#classification -> vocab_size, 출력 위치도 변경\n",
        "\n",
        "class TextClassifier(nn.Module):  ### 여기가 수정부분\n",
        "  def __init__(self, vocab_size, d_model, n_layers, dff):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = nn.parameter.Parameter(positional_encoding(max_len, d_model), requires_grad=False)\n",
        "    self.layers = nn.ModuleList([TransformerLayer(d_model, d_model, dff) for _ in range(n_layers)])\n",
        "    self.classification = nn.Linear(d_model, vocab_size)  # 마지막 단어 분류\n",
        "\n",
        "  def forward(self, x):\n",
        "    mask = (x == tokenizer.pad_token_id)\n",
        "    mask = mask[:, None, :]\n",
        "    seq_len = x.shape[1]\n",
        "\n",
        "    x = self.embedding(x)\n",
        "    x = x * sqrt(self.embedding.embedding_dim)\n",
        "    x = x + self.pos_encoding[:, :seq_len]\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "\n",
        "    x = x[:, -1]  # 마지막 위치만 출력\n",
        "    x = self.classification(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "model = TextClassifier(len(tokenizer), 32, 2, 32)\n",
        "\n",
        "'''\n",
        "class TextClassifier(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, n_layers, dff):\n",
        "    super().__init__()\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.d_model = d_model\n",
        "    self.n_layers = n_layers\n",
        "    self.dff = dff\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    #학습되지 않는 파라미터로 위치정보 고정\n",
        "    self.pos_encoding = nn.parameter.Parameter(positional_encoding(max_len, d_model), requires_grad=False)\n",
        "    self.layers = nn.ModuleList([TransformerLayer(d_model, d_model, dff) for _ in range(n_layers)])\n",
        "    self.classification = nn.Linear(d_model, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    mask = (x == tokenizer.pad_token_id)\n",
        "    mask = mask[:, None, :]\n",
        "    seq_len = x.shape[1]\n",
        "\n",
        "    #정적인 positional encoding을 임베딩에 더해서 사용\n",
        "    #이 방식은 학습 없이도 순서 정보가 모델에 주입됨\n",
        "    x = self.embedding(x)\n",
        "    x = x * sqrt(self.d_model)\n",
        "    x = x + self.pos_encoding[:, :seq_len]\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "\n",
        "    x = x[:, 0]\n",
        "    x = self.classification(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "model = TextClassifier(len(tokenizer), 32, 2, 32)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXpjPWHjbUK8"
      },
      "source": [
        "기존과 다른 점들은 다음과 같습니다:\n",
        "1. `nn.ModuleList`를 사용하여 여러 layer의 구현을 쉽게 하였습니다.\n",
        "2. Embedding, positional encoding, transformer layer를 거치고 난 후 마지막 label을 예측하기 위해 사용한 값은 `x[:, 0]`입니다. 기존의 RNN에서는 padding token을 제외한 마지막 token에 해당하는 representation을 사용한 것과 다릅니다. 이렇게 사용할 수 있는 이유는 attention 과정을 보시면 첫 번째 token에 대한 representation은 이후의 모든 token의 영향을 받습니다. 즉, 첫 번째 token 또한 전체 문장을 대변하는 의미를 가지고 있다고 할 수 있습니다. 그래서 일반적으로 Transformer를 text 분류에 사용할 때는 이와 같은 방식으로 구현됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDq05OlAb2lB"
      },
      "source": [
        "## 학습\n",
        "\n",
        "학습하는 코드는 기존 실습들과 동일하기 때문에 마지막 결과만 살펴보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YHVVsWBPQmnv",
        "outputId": "36d0078b-b661-46ad-8b83-1369d413280d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# BCEWithLogisticLoss를 손실 함수로 사용하고 Adam옵티마이저를 활용하여 학습\\nloss_fn = nn.BCEWithLogitsLoss()\\n'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch.optim import Adam\n",
        "#추가(이건 단어 예측처럼 클래스 개수가 많을 때 (여기선 vocab_size만큼) 사용하는 손실 함수)\n",
        "#수정 전: BCEWithLogitsLoss() ← 이진 분류용\n",
        "#수정 후: CrossEntropyLoss() ← 다중 클래스 분류용 (단어 예측)\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "loss_fn = CrossEntropyLoss() # 손실함수 수정 - BCE -> CrossEntropy\n",
        "\n",
        "lr = 0.001\n",
        "optimizer = Adam(model.parameters(), lr=lr)\n",
        "model = model.to('cuda')\n",
        "\n",
        "\n",
        "'''\n",
        "# BCEWithLogisticLoss를 손실 함수로 사용하고 Adam옵티마이저를 활용하여 학습\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "r88BALxO1zc1",
        "outputId": "0ce6bcd2-ff13-435a-fb9b-c9c351eaa6cc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n#accuracy함수를 정의하여 모델의 정확도 평가\\ndef accuracy(model, dataloader):\\n  cnt = 0\\n  acc = 0\\n\\n  for data in dataloader:\\n    inputs, labels = data\\n    inputs, labels = inputs.to('cuda'), labels.to('cuda')\\n\\n    preds = model(inputs)\\n    # preds = torch.argmax(preds, dim=-1)\\n    preds = (preds > 0).long()[..., 0]\\n\\n    cnt += labels.shape[0]\\n    acc += (labels == preds).sum().item()\\n\\n  return acc / cnt\""
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 정확도 계산도 argmax기준으로 변경\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def accuracy(model, dataloader): #이부분 수정\n",
        "  cnt = 0\n",
        "  acc = 0\n",
        "\n",
        "  for data in dataloader:\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "\n",
        "    preds = model(inputs)\n",
        "    preds = torch.argmax(preds, dim=-1)\n",
        "\n",
        "    acc += (preds == labels).sum().item()\n",
        "    cnt += labels.shape[0]\n",
        "\n",
        "  return acc / cnt\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "#accuracy함수를 정의하여 모델의 정확도 평가\n",
        "def accuracy(model, dataloader):\n",
        "  cnt = 0\n",
        "  acc = 0\n",
        "\n",
        "  for data in dataloader:\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "\n",
        "    preds = model(inputs)\n",
        "    # preds = torch.argmax(preds, dim=-1)\n",
        "    preds = (preds > 0).long()[..., 0]\n",
        "\n",
        "    cnt += labels.shape[0]\n",
        "    acc += (labels == preds).sum().item()\n",
        "\n",
        "  return acc / cnt'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "al_b56TYRILq",
        "outputId": "25cac42e-bd31-44a8-a9d3-9a8f189c2770"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎯 디버깅 출력 시작\n",
            "loss: 10.283707618713379\n",
            "labels dtype: torch.int64\n",
            "labels sample: tensor([6582, 1012, 1012, 1012, 9055], device='cuda:0')\n",
            "preds shape: torch.Size([64, 30522])\n",
            "preds sample: tensor([ 0.2103, -0.1811, -0.1987, -0.0881, -0.1216], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "Epoch   0 | Avg Train Loss: 0.0000\n",
            "Epoch   1 | Avg Train Loss: 3.6570\n",
            "Epoch   2 | Avg Train Loss: 2.7941\n",
            "Epoch   3 | Avg Train Loss: 2.6600\n",
            "Epoch   4 | Avg Train Loss: 2.6107\n",
            "Epoch   5 | Avg Train Loss: 2.5806\n",
            "Epoch   6 | Avg Train Loss: 2.5560\n",
            "Epoch   7 | Avg Train Loss: 2.5413\n",
            "Epoch   8 | Avg Train Loss: 2.5074\n",
            "Epoch   9 | Avg Train Loss: 2.4840\n",
            "Epoch  10 | Avg Train Loss: 2.4511\n",
            "Epoch  11 | Avg Train Loss: 2.4179\n",
            "Epoch  12 | Avg Train Loss: 2.3844\n",
            "Epoch  13 | Avg Train Loss: 2.3512\n",
            "Epoch  14 | Avg Train Loss: 2.3202\n",
            "Epoch  15 | Avg Train Loss: 2.2787\n",
            "Epoch  16 | Avg Train Loss: 2.2463\n",
            "Epoch  17 | Avg Train Loss: 2.2147\n",
            "Epoch  18 | Avg Train Loss: 2.1844\n",
            "Epoch  19 | Avg Train Loss: 2.1407\n",
            "Epoch  20 | Avg Train Loss: 2.1082\n",
            "Epoch  21 | Avg Train Loss: 2.0719\n",
            "Epoch  22 | Avg Train Loss: 2.0549\n",
            "Epoch  23 | Avg Train Loss: 2.0233\n",
            "Epoch  24 | Avg Train Loss: 1.9918\n",
            "Epoch  25 | Avg Train Loss: 1.9756\n",
            "Epoch  26 | Avg Train Loss: 1.9440\n",
            "Epoch  27 | Avg Train Loss: 1.9175\n",
            "Epoch  28 | Avg Train Loss: 1.8917\n",
            "Epoch  29 | Avg Train Loss: 1.8763\n",
            "Epoch  30 | Avg Train Loss: 1.8356\n",
            "Epoch  31 | Avg Train Loss: 1.8097\n",
            "Epoch  32 | Avg Train Loss: 1.7985\n",
            "Epoch  33 | Avg Train Loss: 1.7903\n",
            "Epoch  34 | Avg Train Loss: 1.7506\n",
            "Epoch  35 | Avg Train Loss: 1.7210\n",
            "Epoch  36 | Avg Train Loss: 1.7153\n",
            "Epoch  37 | Avg Train Loss: 1.7002\n",
            "Epoch  38 | Avg Train Loss: 1.6510\n",
            "Epoch  39 | Avg Train Loss: 1.6584\n",
            "Epoch  40 | Avg Train Loss: 1.5979\n",
            "Epoch  41 | Avg Train Loss: 1.5690\n",
            "Epoch  42 | Avg Train Loss: 1.5484\n",
            "Epoch  43 | Avg Train Loss: 1.5417\n",
            "Epoch  44 | Avg Train Loss: 1.5280\n",
            "Epoch  45 | Avg Train Loss: 1.5391\n",
            "Epoch  46 | Avg Train Loss: 1.4963\n",
            "Epoch  47 | Avg Train Loss: 1.4672\n",
            "Epoch  48 | Avg Train Loss: 1.4182\n",
            "Epoch  49 | Avg Train Loss: 1.4197\n",
            "=========> Train acc: 0.670 | Test acc: 0.518\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "n_epochs = 50\n",
        "train_losses = [] #에폭별 평균 loss 저장용 리스트\n",
        "\n",
        "#학습루프 부분 수정\n",
        "for epoch in range(n_epochs):\n",
        "  total_loss = 0.\n",
        "  model.train()\n",
        "  for data in train_loader:\n",
        "    model.zero_grad()\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "\n",
        "    preds = model(inputs)\n",
        "    loss = loss_fn(preds, labels)\n",
        "\n",
        "\n",
        "\n",
        "    # ✅ 여기부터 디버깅용 출력\n",
        "    if epoch == 0 and total_loss == 0:  # 첫 에폭의 첫 배치만 출력\n",
        "        print(\"🎯 디버깅 출력 시작\")\n",
        "        print(\"loss:\", loss.item())\n",
        "        print(\"labels dtype:\", labels.dtype)\n",
        "        print(\"labels sample:\", labels[:5])\n",
        "        print(\"preds shape:\", preds.shape)\n",
        "        print(\"preds sample:\", preds[0][:5])\n",
        "        break  # 첫 배치만 확인\n",
        "\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  #loss 그래프 시각화\n",
        "  avg_loss = total_loss / len(train_loader)  # 🔸 평균 loss 계산\n",
        "  train_losses.append(avg_loss)  # 🔸 loss 저장\n",
        "\n",
        "  print(f\"Epoch {epoch:3d} | Avg Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# 결과출력\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  train_acc = accuracy(model, train_loader)\n",
        "  test_acc = accuracy(model, test_loader)\n",
        "  print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")\n",
        "\n",
        "'''\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    train_acc = accuracy(model, train_loader)\n",
        "    test_acc = accuracy(model, test_loader)\n",
        "    print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")\n",
        "'''\n",
        "\n",
        "# 🔻 학습 loss 시각화\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_losses, marker='o')\n",
        "plt.title(\"Training Loss Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqZays2yb8Ja"
      },
      "source": [
        "학습이 안정적으로 진행되며 RNN보다 빨리 수렴하는 것을 확인할 수 있습니다.\n",
        "하지만 test 정확도가 RNN보다 낮은 것을 보았을 때, overfitting에 취약하다는 것을 알 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "u-I121osywgW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
